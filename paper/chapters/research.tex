The given research project studies the usage of hybrid quantum-classical deep \ac{RL} algorithms and occuring instabilities in their usage.
\ac{RL} is one field in machine learning.
The main idea is to train an agent without further instructions, so the agent has to find out which actions and general policies result in the highest reward.
One sub class of \ac{RL} is Q-learning, following the idea of approximating directly the optimal action-value function based on the learned action-value function.\autocite{rl}
At this point, quantum computing with its computational speedup potentials comes in, for example in quantum mechanical algorithms for database search\autocite{databasesearch}. 

% TODO: use in repro.tex for describing choice of split up of one click dispatcher
% Deep \ac{RL} approaches have hardware intense requirements for achieving and outperforming human benchmarks like described by Badia et al. for exceeding the human benchmark in Atari games.\autocite{atari}


The proposed strategy for this research is the usage of \ac{VQ-DQN} described by Chen et al. so classical deep \ac{RL} algorithms for von Neumann architectures have a quantum computing representation.\autocite{vqdqn},
For this purpose, variational quantum circuits are used to create a quantum equivalent of deep Q-learning.
Q-learning follows the idea autocite{rl}
Quantum deep Q-networks replace the classical neural network with variational quantum circuits.
Those variational quantum circuits follow a design of a fixed structure of gates, operating on a set of qubits.\autocite{circuits}


For the usage of \ac{VQ-DQN}, it is necessary to map a state of the classical markov deciscion process to a quantum state. 
Lockwood and Si use Scaled encoding and Directional encoding.\autocite{lockwood}
Skolik et al. add Continous encoding to those possibilities.\autocite{skolik}


Based on the research and reproduction study of Franz et al.\autocite{instabilities}, we reconstruct the reproduced training process of Lockwood and Si\autocite{lockwood} and Skolik et al.\autocite{skolik} regarding the training of \ac{VQ-DQN} agents on the CartPole-v1 task. 
We use the approaches of Continuous, Scaled \& Continuous and Scaled \& Directional encoding and the Q-value extraction methods Local Scaling, Global Scaling and Global Scaling with Quantum Pooling like descirbed by Franz et al.\autocite{instabilities}.
