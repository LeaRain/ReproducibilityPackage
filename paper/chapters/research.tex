The given research project studies the usage of hybrid quantum-classical deep \ac{RL} algorithms and occuring instabilities in their usage.
\ac{RL} is one field in machine learning.
The main idea is to train an agent without further instructions, so the agent has to find out which actions and general policies result in the highest reward.
One sub class of \ac{RL} is Q-learning, following the idea of approximating directly the optimal action-value function based on the learned action-value function.\autocite{rl} 
Q-learning can be applied to quantum computing as well.


The proposed strategy for this research is the usage of \ac{VQ-DQN} described by Chen et al. so classical deep \ac{RL} algorithms for von Neumann architectures have a quantum computing representation.\autocite{vqdqn},
For this purpose, variational quantum circuits are used to create a quantum equivalent of deep Q-learning.
Quantum deep Q-networks replace the classical neural network with variational quantum circuits.
Those circuits follow a design of a fixed structure of gates, operating on a set of qubits.\autocite{circuits}


For the usage of \ac{VQ-DQN}, it is necessary to map a state of the classical markov deciscion process to a quantum state by the usage of the qubits in the variational quantum circuit. 
Lockwood and Si use Scaled encoding and Directional encoding.\autocite{lockwood}
Skolik et al. add Continous encoding to those possibilities.\autocite{skolik} 
The different encoding strategies describe different rotation policies for the specific qubits.


Based on the research and reproduction study of Franz et al., we reconstruct the reproduced training process of Lockwood and Si and Skolik et al. regarding the training of \ac{VQ-DQN} agents on the CartPole task. 
We use the approaches of Continuous (continuous for all input parameters), Scaled \& Continuous (scaled for finite-domain input parameters, continuous for rest) and Scaled \& Directional (scaled for finite-domain input parameters, directional for rest) encoding. 
For the Q-value extraction methods, we use Local Scaling (scaling of the output by a dedicated trainable weight), Global Scaling (scaling of all outputs by one trainable weight) and Global Scaling with Quantum Pooling (quantum pooling with following global scaling) like descirbed by Franz et al.
The results can be found in Figure \ref{results}: For every extraction strategy every coding is used for five runs each.
The validation return is averaged over those five runs.

\begin{figure*}[tb]
\centering
	\scalebox{.62}{\input{comparison.pgf}}
	\caption{Returns of the validation process (averaged over five runs each)  with the usage of \ac{VQ-DQN} described reproduced by Franz et al.\autocite{instabilities}, originally used by Lockwood and Si\autocite{lockwood} and Skolik et al.\autocite{skolik} with the corresponding extraction strategy.}
\label{results}
\end{figure*}

The result of the reproducibility experiments of Franz et al. show instabilities in every run, independent of the structure, encoding and extraction method. 
So it is not a surprise that we are not able to reproduce the exact same results, which also applies to the original results.
In fact, we reproduce that we cannot reproduce the same results, which is also the observation of Franz et al. 
